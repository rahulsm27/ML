{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-26T11:44:01.719269Z","iopub.execute_input":"2023-11-26T11:44:01.720100Z","iopub.status.idle":"2023-11-26T11:44:20.064440Z","shell.execute_reply.started":"2023-11-26T11:44:01.720046Z","shell.execute_reply":"2023-11-26T11:44:20.063143Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Collecting numpy==1.23.0\n  Downloading numpy-1.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.0/17.0 MB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: numpy\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.24.3\n    Uninstalling numpy-1.24.3:\n      Successfully uninstalled numpy-1.24.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.7 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\nchex 0.1.84 requires numpy>=1.24.1, but you have numpy 1.23.0 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2023.11.0 which is incompatible.\ncuml 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.11.0 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2023.11.0 which is incompatible.\ndask-cuda 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.11.0 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2023.11.0 which is incompatible.\ndask-cudf 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.11.0 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\nlibpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.6.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\npymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 1.23.0 which is incompatible.\npymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.11.3 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2023.11.0 which is incompatible.\nraft-dask 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.11.0 which is incompatible.\ntensorflowjs 4.13.0 requires packaging~=23.1, but you have packaging 21.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed numpy-1.23.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 1. INSTALL LIBRARIES AND PACKAGES","metadata":{}},{"cell_type":"code","source":"\n# Import the necessary libraries\n!pip install datasets\n!pip install transformers\n!pip install torch\n!pip install evaluate\n!pip install jiwer\n!pip install transformers[torch]\n!pip install numpy\n!pip install numpy==1.22.0\n","metadata":{"execution":{"iopub.status.busy":"2023-11-26T11:37:32.573321Z","iopub.execute_input":"2023-11-26T11:37:32.573604Z","iopub.status.idle":"2023-11-26T11:38:57.707973Z","shell.execute_reply.started":"2023-11-26T11:37:32.573578Z","shell.execute_reply":"2023-11-26T11:38:57.706924Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.1.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.24.3)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (11.0.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.7)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.0.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.15)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2023.10.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.8.5)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.17.3)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.18.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (3.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.12.2)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2023.7.22)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.35.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.17.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.24.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.8.8)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers<0.15,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.14.1)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.10.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.7.22)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.0.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nCollecting evaluate\n  Obtaining dependency information for evaluate from https://files.pythonhosted.org/packages/70/63/7644a1eb7b0297e585a6adec98ed9e575309bb973c33b394dae66bc35c69/evaluate-0.4.1-py3-none-any.whl.metadata\n  Downloading evaluate-0.4.1-py3-none-any.whl.metadata (9.4 kB)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.1.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.24.3)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.7)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.0.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.15)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2023.10.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.17.3)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.18.0)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (11.0.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.8.5)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.12.2)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2023.7.22)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nDownloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.1\nCollecting jiwer\n  Obtaining dependency information for jiwer from https://files.pythonhosted.org/packages/0d/4f/ee537ab20144811dd99321735ff92ef2b3a3230b77ed7454bed4c44d21fc/jiwer-3.0.3-py3-none-any.whl.metadata\n  Downloading jiwer-3.0.3-py3-none-any.whl.metadata (2.6 kB)\nRequirement already satisfied: click<9.0.0,>=8.1.3 in /opt/conda/lib/python3.10/site-packages (from jiwer) (8.1.7)\nRequirement already satisfied: rapidfuzz<4,>=3 in /opt/conda/lib/python3.10/site-packages (from jiwer) (3.5.2)\nDownloading jiwer-3.0.3-py3-none-any.whl (21 kB)\nInstalling collected packages: jiwer\nSuccessfully installed jiwer-3.0.3\nRequirement already satisfied: transformers[torch] in /opt/conda/lib/python3.10/site-packages (4.35.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.17.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (1.24.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2023.8.8)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2.31.0)\nRequirement already satisfied: tokenizers<0.15,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.14.1)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.4.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (4.66.1)\nRequirement already satisfied: torch!=1.12.0,>=1.10 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2.0.0)\nRequirement already satisfied: accelerate>=0.20.3 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.24.1)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.20.3->transformers[torch]) (5.9.3)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]) (2023.10.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers[torch]) (3.0.9)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (2023.7.22)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch!=1.12.0,>=1.10->transformers[torch]) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch!=1.12.0,>=1.10->transformers[torch]) (1.3.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (1.24.3)\n","output_type":"stream"}]},{"cell_type":"code","source":"##Imports required\nimport numpy as np\nfrom datasets import load_dataset, Audio, DatasetDict\nimport torch\nimport evaluate\nfrom dataclasses import dataclass, field\nfrom typing import Any, Dict, List, Optional, Union\nfrom transformers import Seq2SeqTrainingArguments,Seq2SeqTrainer\n#from transformers import TrainingArguments, Trainer\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2023-11-26T11:41:34.802631Z","iopub.execute_input":"2023-11-26T11:41:34.803062Z","iopub.status.idle":"2023-11-26T11:41:54.510754Z","shell.execute_reply.started":"2023-11-26T11:41:34.803021Z","shell.execute_reply":"2023-11-26T11:41:54.509713Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"name":"stdout","text":"cuda\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 2. IMPORT DATA","metadata":{}},{"cell_type":"code","source":"dataset = DatasetDict()\n\n# Load the PolyAI dataset.\n\ndataset = load_dataset(\"PolyAI/minds14\", name=\"en-US\", split=\"train[:80]\")\n\n# Remove unnecessary columns\n\n\n# Split the datasedataset = \ndataset.remove_columns(['path','english_transcription','intent_class','lang_id'])\n\n#split into train and test\ndataset = dataset.train_test_split(test_size = 0.2, shuffle=False)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-26T11:43:04.168322Z","iopub.execute_input":"2023-11-26T11:43:04.169302Z","iopub.status.idle":"2023-11-26T11:43:04.606593Z","shell.execute_reply.started":"2023-11-26T11:43:04.169253Z","shell.execute_reply":"2023-11-26T11:43:04.605863Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"#dataset['train']['audio'][0]","metadata":{"execution":{"iopub.status.busy":"2023-11-26T11:41:54.525519Z","iopub.status.idle":"2023-11-26T11:41:54.526047Z","shell.execute_reply.started":"2023-11-26T11:41:54.525779Z","shell.execute_reply":"2023-11-26T11:41:54.525803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Resample the dataset to 16 Khz as MCTCT model is trained on 16khz\ndataset['train'] = dataset['train'].cast_column(\"audio\", Audio(sampling_rate=16000))\ndataset['test'] = dataset['test'].cast_column(\"audio\", Audio(sampling_rate=16000))\n# #dataset['train'] = dataset['train'].rename_column(\"audio\", Audio(sampling_rate=16000))\n# dataset['train'] = dataset['train'].rename_column(\"transcription\",\"sentence\")\n# dataset['test'] = dataset['test'].rename_column(\"transcription\",\"sentence\")","metadata":{"execution":{"iopub.status.busy":"2023-11-26T11:44:22.453969Z","iopub.execute_input":"2023-11-26T11:44:22.454992Z","iopub.status.idle":"2023-11-26T11:44:22.469172Z","shell.execute_reply.started":"2023-11-26T11:44:22.454947Z","shell.execute_reply":"2023-11-26T11:44:22.468075Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"dataset","metadata":{"execution":{"iopub.status.busy":"2023-11-26T11:44:25.573416Z","iopub.execute_input":"2023-11-26T11:44:25.573784Z","iopub.status.idle":"2023-11-26T11:44:25.581587Z","shell.execute_reply.started":"2023-11-26T11:44:25.573754Z","shell.execute_reply":"2023-11-26T11:44:25.580703Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['path', 'audio', 'transcription', 'english_transcription', 'intent_class', 'lang_id'],\n        num_rows: 64\n    })\n    test: Dataset({\n        features: ['path', 'audio', 'transcription', 'english_transcription', 'intent_class', 'lang_id'],\n        num_rows: 16\n    })\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"## 3. DATA PREPROCESSING","metadata":{}},{"cell_type":"code","source":"from transformers import WhisperProcessor, WhisperForConditionalGeneration\nprocessor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny.en\",task=\"transcribe\", model_max_length=225)\nmodel = WhisperForConditionalGeneration.from_pretrained('openai/whisper-tiny.en')\nmodel.to(device)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-26T11:44:37.661609Z","iopub.execute_input":"2023-11-26T11:44:37.662364Z","iopub.status.idle":"2023-11-26T11:44:46.456563Z","shell.execute_reply.started":"2023-11-26T11:44:37.662330Z","shell.execute_reply":"2023-11-26T11:44:46.455584Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)rocessor_config.json:   0%|          | 0.00/185k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0eb857c02a64982b2e6890cc44e5d86"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer_config.json:   0%|          | 0.00/805 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d94f7474331c4e6282573d7de3c4fc60"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7bd9fc62713249688f88e382c4c8dc91"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer.json:   0%|          | 0.00/2.41M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b466d1c83c44ac2a56883617ea7d943"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"016d4906be4d4bd8becee5c9cbf6eb29"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading normalizer.json:   0%|          | 0.00/52.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb43f873a0024111a2e936052ef342cd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading added_tokens.json:   0%|          | 0.00/34.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70c02910e341475bb6a0f97b588ae819"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/1.83k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"147ede61e1184d9a8fb41de0106d677b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading config.json:   0%|          | 0.00/1.94k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1387df06223429bae0c395da5432e2a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/151M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07fd9e62779d4009b3041e4b3563735a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading generation_config.json:   0%|          | 0.00/1.59k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"caed8c00ddb6489f8125eb1363d2c17e"}},"metadata":{}},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"WhisperForConditionalGeneration(\n  (model): WhisperModel(\n    (encoder): WhisperEncoder(\n      (conv1): Conv1d(80, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n      (conv2): Conv1d(384, 384, kernel_size=(3,), stride=(2,), padding=(1,))\n      (embed_positions): Embedding(1500, 384)\n      (layers): ModuleList(\n        (0-3): 4 x WhisperEncoderLayer(\n          (self_attn): WhisperAttention(\n            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n            (v_proj): Linear(in_features=384, out_features=384, bias=True)\n            (q_proj): Linear(in_features=384, out_features=384, bias=True)\n            (out_proj): Linear(in_features=384, out_features=384, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n          (activation_fn): GELUActivation()\n          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n          (final_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n    )\n    (decoder): WhisperDecoder(\n      (embed_tokens): Embedding(51864, 384, padding_idx=50256)\n      (embed_positions): WhisperPositionalEmbedding(448, 384)\n      (layers): ModuleList(\n        (0-3): 4 x WhisperDecoderLayer(\n          (self_attn): WhisperAttention(\n            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n            (v_proj): Linear(in_features=384, out_features=384, bias=True)\n            (q_proj): Linear(in_features=384, out_features=384, bias=True)\n            (out_proj): Linear(in_features=384, out_features=384, bias=True)\n          )\n          (activation_fn): GELUActivation()\n          (self_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): WhisperAttention(\n            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n            (v_proj): Linear(in_features=384, out_features=384, bias=True)\n            (q_proj): Linear(in_features=384, out_features=384, bias=True)\n            (out_proj): Linear(in_features=384, out_features=384, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n          (final_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n  (proj_out): Linear(in_features=384, out_features=51864, bias=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"# Preparing a function to process the entire dataset\n# We need to crate two variables with name 'input_featrues'(input array of sound wave in raw foram) and 'labels'(transcription)\n\ndef prepare_dataset(batch):\n    audio = batch[\"audio\"]\n\n   # batch[\"input_ids\"] = processor.feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"],return_tensor = \"pt\").input_features[0]\n    batch[\"input_features\"] = processor.feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"],return_tensor = \"pt\").input_features[0]\n\n    #processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n  #  with processor.as_target_processor():\n    batch[\"labels\"] = processor.tokenizer(batch[\"transcription\"]).input_ids\n    return batch","metadata":{"execution":{"iopub.status.busy":"2023-11-26T11:45:25.820324Z","iopub.execute_input":"2023-11-26T11:45:25.820697Z","iopub.status.idle":"2023-11-26T11:45:25.827123Z","shell.execute_reply.started":"2023-11-26T11:45:25.820660Z","shell.execute_reply":"2023-11-26T11:45:25.825998Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"np.object = object    \nencoded_dataset = dataset.map(prepare_dataset,num_proc=4)","metadata":{"execution":{"iopub.status.busy":"2023-11-26T11:46:32.132125Z","iopub.execute_input":"2023-11-26T11:46:32.133080Z","iopub.status.idle":"2023-11-26T11:46:49.988240Z","shell.execute_reply.started":"2023-11-26T11:46:32.133036Z","shell.execute_reply":"2023-11-26T11:46:49.987032Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"       ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#0:   0%|          | 0/16 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec3ce51e3f944e398ffd63646c5980b0"}},"metadata":{}},{"name":"stdout","text":" ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#1:   0%|          | 0/16 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"014987f16fdd438fbf4e7fd681383c78"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"#2:   0%|          | 0/16 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a97b3ce20d864541956b4ac2a5885fd5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"#3:   0%|          | 0/16 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a86ac12eafe4ac9b9887aea79771f5a"}},"metadata":{}},{"name":"stdout","text":"      ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#0:   0%|          | 0/4 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2be8a197969b4f019b35543a35dcce0b"}},"metadata":{}},{"name":"stdout","text":" ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#1:   0%|          | 0/4 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a830bfe062be4c8f93efd4a059856f36"}},"metadata":{}},{"name":"stdout","text":" ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#3:   0%|          | 0/4 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70edf84a86eb40159ebd07287b9cb5c0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"#2:   0%|          | 0/4 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55510b6760664d3d9c78c9e6f3f61da3"}},"metadata":{}}]},{"cell_type":"code","source":"encoded_dataset","metadata":{"execution":{"iopub.status.busy":"2023-11-26T11:46:49.990216Z","iopub.execute_input":"2023-11-26T11:46:49.990542Z","iopub.status.idle":"2023-11-26T11:46:49.999085Z","shell.execute_reply.started":"2023-11-26T11:46:49.990508Z","shell.execute_reply":"2023-11-26T11:46:49.996508Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['path', 'audio', 'transcription', 'english_transcription', 'intent_class', 'lang_id', 'input_features', 'labels'],\n        num_rows: 64\n    })\n    test: Dataset({\n        features: ['path', 'audio', 'transcription', 'english_transcription', 'intent_class', 'lang_id', 'input_features', 'labels'],\n        num_rows: 16\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"# Creating a DataCollatorClass\n\nfrom dataclasses import dataclass, field\nfrom typing import Any, Dict, List, Optional, Union\n\n\n@dataclass\nclass DataCollatorSpeechSeq2SeqWithPadding:\n    processor: processor\n    padding: Union[bool, str] = \"longest\"\n\n    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n#     def __call__(self, features):# List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n        # split inputs and labels since they have to be of different lengths and need\n        # different padding methods\n       # print(features)\n        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n\n        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n        \n        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n\n        # replace padding with -100 to ignore loss correctly\n        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n\n        batch[\"labels\"] = labels\n       # print(batch)\n        return batch #batch\n","metadata":{"execution":{"iopub.status.busy":"2023-11-26T11:46:50.000210Z","iopub.execute_input":"2023-11-26T11:46:50.000479Z","iopub.status.idle":"2023-11-26T11:46:50.027205Z","shell.execute_reply.started":"2023-11-26T11:46:50.000455Z","shell.execute_reply":"2023-11-26T11:46:50.026360Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"\ndata_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-26T11:46:50.028743Z","iopub.execute_input":"2023-11-26T11:46:50.029059Z","iopub.status.idle":"2023-11-26T11:46:50.040515Z","shell.execute_reply.started":"2023-11-26T11:46:50.029035Z","shell.execute_reply":"2023-11-26T11:46:50.039645Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"\n# Evalution metric- We will be evaluating our model on word error rate\n\nimport evaluate\n\nmetric = evaluate.load(\"wer\")\n\n\ndef compute_metrics(pred):\n #   wer = evaluate.load(\"wer\")\n    pred_ids = pred.predictions\n    label_ids = pred.label_ids\n\n    # replace -100 with the pad_token_id\n    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n\n    # we do not want to group tokens when computing the metrics\n    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n\n    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n\n    return {\"wer\": wer}\n","metadata":{"execution":{"iopub.status.busy":"2023-11-26T11:47:45.845943Z","iopub.execute_input":"2023-11-26T11:47:45.846327Z","iopub.status.idle":"2023-11-26T11:47:46.440538Z","shell.execute_reply.started":"2023-11-26T11:47:45.846295Z","shell.execute_reply":"2023-11-26T11:47:46.439680Z"},"trusted":true},"execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/4.49k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5fee22989b104d4195402d19ed3b1ab9"}},"metadata":{}}]},{"cell_type":"markdown","source":"#### 4. Training the model","metadata":{}},{"cell_type":"code","source":"\nmodel.config.forced_decoder_ids = None\nmodel.config.suppress_tokens = []\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"seqtoseq-trained\",\n    gradient_checkpointing=True,\n    per_device_train_batch_size=2,\n    learning_rate=1e-5,\n    warmup_steps=2,\n    max_steps=2000,\n    fp16=True ,#False ,#True,\n    optim='adafactor',\n   # group_by_length=True,\n    predict_with_generate=True,\n    evaluation_strategy=\"steps\",\n    per_device_eval_batch_size=2,\n    eval_steps=100,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"wer\",\n    report_to = [\"tensorboard\"],\n    #data_parallel=False\n)\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=encoded_dataset[\"train\"],\n    eval_dataset=encoded_dataset[\"test\"],\n    tokenizer=processor,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n    #data_parallel=False\n  #  sampler = None\n)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-26T11:47:50.025837Z","iopub.execute_input":"2023-11-26T11:47:50.026552Z","iopub.status.idle":"2023-11-26T11:47:50.063796Z","shell.execute_reply.started":"2023-11-26T11:47:50.026517Z","shell.execute_reply":"2023-11-26T11:47:50.063085Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# Requires GPU for training\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-11-26T11:47:52.198766Z","iopub.execute_input":"2023-11-26T11:47:52.199647Z","iopub.status.idle":"2023-11-26T12:32:51.419502Z","shell.execute_reply.started":"2023-11-26T11:47:52.199612Z","shell.execute_reply":"2023-11-26T12:32:51.418658Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2000' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2000/2000 44:46, Epoch 125/125]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Wer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>No log</td>\n      <td>0.661608</td>\n      <td>62.903226</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>No log</td>\n      <td>0.208757</td>\n      <td>374.193548</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>No log</td>\n      <td>0.233541</td>\n      <td>2024.193548</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>No log</td>\n      <td>0.337271</td>\n      <td>2896.774194</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.223400</td>\n      <td>0.382448</td>\n      <td>25.806452</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.223400</td>\n      <td>0.394426</td>\n      <td>479.032258</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.223400</td>\n      <td>0.416275</td>\n      <td>585.483871</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.223400</td>\n      <td>0.466375</td>\n      <td>674.193548</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.223400</td>\n      <td>0.468723</td>\n      <td>845.161290</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.000000</td>\n      <td>0.469956</td>\n      <td>908.064516</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.000000</td>\n      <td>0.500571</td>\n      <td>354.838710</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.000000</td>\n      <td>0.496664</td>\n      <td>273.387097</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>0.000000</td>\n      <td>0.511112</td>\n      <td>108.870968</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.000000</td>\n      <td>0.521375</td>\n      <td>327.419355</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.000000</td>\n      <td>0.519362</td>\n      <td>417.741935</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>0.000000</td>\n      <td>0.522389</td>\n      <td>194.354839</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>0.000000</td>\n      <td>0.524534</td>\n      <td>190.322581</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>0.000000</td>\n      <td>0.528309</td>\n      <td>116.935484</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>0.000000</td>\n      <td>0.528803</td>\n      <td>190.322581</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.000000</td>\n      <td>0.529339</td>\n      <td>190.322581</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nThere were missing keys in the checkpoint model loaded: ['proj_out.weight'].\n","output_type":"stream"},{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=2000, training_loss=0.05585742835774909, metrics={'train_runtime': 2698.7692, 'train_samples_per_second': 2.964, 'train_steps_per_second': 0.741, 'total_flos': 1.9695108096e+17, 'train_loss': 0.05585742835774909, 'epoch': 125.0})"},"metadata":{}}]},{"cell_type":"code","source":"## getting test data\ni2 = processor(dataset['test'][6][\"audio\"][\"array\"], sampling_rate=16000, return_tensors=\"pt\")\nprint(f\"The input test audio is: {dataset['test'][8]['transcription']}\")\n\ngenerated_ids = model.generate(inputs=input_features)\n# prediction for test data\nwith torch.no_grad():\n    logits = model(**i2.to(device)).logits\n\npredicted_ids = torch.argmax(logits, dim=-1)\ntranscription = processor.batch_decode(predicted_ids)\nprint(f'The output prediction is : {transcription[0]}')\n     ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## getting test data\ninputs = processor(dataset['test'][8][\"audio\"][\"array\"], sampling_rate=16000, return_tensors=\"pt\").to(device).input_features\nprint(f\"The input test audio is: {dataset['test'][8]['transcription']}\")\n\ngenerated_ids = model.generate(inputs=inputs)\n# prediction for test data\n# with torch.no_grad():\n#     logits = model(**i2.to(device)).logits\n\n#predicted_ids = torch.argmax(logits, dim=-1)\ntranscription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\nprint(f'The output prediction is : {transcription}')\n     ","metadata":{"execution":{"iopub.status.busy":"2023-11-26T12:42:25.483447Z","iopub.execute_input":"2023-11-26T12:42:25.484427Z","iopub.status.idle":"2023-11-26T12:42:25.699221Z","shell.execute_reply.started":"2023-11-26T12:42:25.484389Z","shell.execute_reply":"2023-11-26T12:42:25.698204Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"The input test audio is: how much do I have in my account\nThe output prediction is :  I'm much do I have in my account\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}