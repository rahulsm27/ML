{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNXHcL5j8zgGQWpSJimYweV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahulsm27/ML/blob/main/Newton_Method_for_Neural_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SzfDHOW9ID9I"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn,optim\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
      ],
      "metadata": {
        "id": "V6P_7CeWIGlz"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating our own ConvNet\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 1000)\n",
        "        self.fc2 = nn.Linear(1000, 10)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "model = ConvNet().to(device)\n",
        "torch.nn.init.xavier_uniform_(model.conv1.weight)\n",
        "torch.nn.init.xavier_uniform_(model.conv2.weight)\n"
      ],
      "metadata": {
        "id": "LM9gGCNQIODF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5181b304-8e06-4a97-bd44-a3ccdcf092a9"
      },
      "execution_count": 313,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[[[-0.0562,  0.0806, -0.0634,  0.0177, -0.0316],\n",
              "          [-0.0435,  0.0756, -0.0143,  0.0510,  0.0132],\n",
              "          [-0.0791, -0.0168, -0.0100, -0.0752,  0.0842],\n",
              "          [ 0.0942, -0.0933,  0.0033,  0.0166, -0.0523],\n",
              "          [-0.0627,  0.0404,  0.0753, -0.0465, -0.0319]],\n",
              "\n",
              "         [[ 0.0064,  0.0204, -0.0382,  0.0957,  0.0203],\n",
              "          [ 0.0852,  0.0372,  0.0685, -0.0936, -0.0037],\n",
              "          [ 0.0002, -0.0031, -0.0651, -0.0990, -0.0370],\n",
              "          [-0.0496, -0.0905,  0.0451,  0.0843, -0.0990],\n",
              "          [-0.0979, -0.0839,  0.0102, -0.0869, -0.0248]],\n",
              "\n",
              "         [[-0.0057, -0.0373, -0.0339, -0.0249,  0.0987],\n",
              "          [ 0.0215, -0.0191, -0.0602,  0.0077,  0.0968],\n",
              "          [ 0.0145,  0.0719, -0.0631, -0.0027, -0.0089],\n",
              "          [ 0.0338,  0.0720,  0.0027, -0.0958, -0.0997],\n",
              "          [-0.0817,  0.0081,  0.0611, -0.0911, -0.0872]],\n",
              "\n",
              "         [[-0.0150,  0.0743,  0.0557,  0.0465,  0.0227],\n",
              "          [-0.0504,  0.0464,  0.0592,  0.0701, -0.0417],\n",
              "          [ 0.0826,  0.0851,  0.0167, -0.0042, -0.0704],\n",
              "          [ 0.0778, -0.0518, -0.0754,  0.1017, -0.0404],\n",
              "          [-0.0448, -0.0315,  0.0709, -0.0975, -0.0385]],\n",
              "\n",
              "         [[-0.0335, -0.0178, -0.0661,  0.0333,  0.0459],\n",
              "          [ 0.0991, -0.0104, -0.0433, -0.0357,  0.0374],\n",
              "          [ 0.0058, -0.0649, -0.0108,  0.0369,  0.0057],\n",
              "          [ 0.0017, -0.0381,  0.0207, -0.0601,  0.0431],\n",
              "          [-0.0471,  0.0018,  0.0624, -0.0980,  0.0483]],\n",
              "\n",
              "         [[ 0.0565,  0.0959,  0.0844,  0.0316,  0.0946],\n",
              "          [ 0.0882,  0.0913,  0.1000,  0.0314, -0.0644],\n",
              "          [-0.0756, -0.0267,  0.0601,  0.0636,  0.0687],\n",
              "          [-0.0187,  0.1021,  0.0216,  0.0535, -0.0183],\n",
              "          [-0.0388,  0.0506, -0.0912,  0.0029, -0.1004]]],\n",
              "\n",
              "\n",
              "        [[[ 0.0263, -0.0990, -0.0379, -0.0636,  0.0414],\n",
              "          [-0.0361, -0.1017, -0.0095,  0.0037, -0.0412],\n",
              "          [-0.0988,  0.0667, -0.0731,  0.0410, -0.0980],\n",
              "          [-0.0712,  0.0374, -0.0490,  0.0430, -0.0597],\n",
              "          [ 0.0832,  0.0612, -0.0797,  0.0321, -0.0917]],\n",
              "\n",
              "         [[-0.0526, -0.0297, -0.0999, -0.0513,  0.0563],\n",
              "          [-0.0563,  0.0985,  0.0112,  0.0620, -0.0350],\n",
              "          [-0.0274,  0.0112, -0.0788, -0.0173, -0.0657],\n",
              "          [-0.0115,  0.0934, -0.0431,  0.0911, -0.0644],\n",
              "          [ 0.0782,  0.0975,  0.0771, -0.0838,  0.0976]],\n",
              "\n",
              "         [[-0.0082,  0.0249,  0.0876,  0.0064,  0.0926],\n",
              "          [-0.0281, -0.0261, -0.0258,  0.0177,  0.0032],\n",
              "          [-0.0477, -0.0034,  0.0309, -0.0728, -0.0081],\n",
              "          [-0.0169,  0.0423,  0.0123, -0.0733, -0.0035],\n",
              "          [-0.0575, -0.0558, -0.0928, -0.1023, -0.0581]],\n",
              "\n",
              "         [[-0.0158, -0.0934,  0.0606, -0.0742, -0.1042],\n",
              "          [-0.0892, -0.0405, -0.0964,  0.0781, -0.0889],\n",
              "          [ 0.0400,  0.0304,  0.0603,  0.0611, -0.0361],\n",
              "          [ 0.0953, -0.0063,  0.0059, -0.0020, -0.0124],\n",
              "          [-0.0808,  0.0783, -0.0891, -0.0845, -0.0782]],\n",
              "\n",
              "         [[-0.0417,  0.0868,  0.0246, -0.0515, -0.0924],\n",
              "          [ 0.0052,  0.0360, -0.0820,  0.0748, -0.0404],\n",
              "          [ 0.0947, -0.0368, -0.0135, -0.0264, -0.1018],\n",
              "          [-0.0545, -0.0102,  0.0252,  0.0184,  0.0766],\n",
              "          [-0.0259,  0.0367, -0.0709,  0.0671,  0.0571]],\n",
              "\n",
              "         [[ 0.0403, -0.0252,  0.0969,  0.0645, -0.0121],\n",
              "          [-0.0147,  0.0367,  0.0075, -0.0202,  0.0108],\n",
              "          [-0.0997, -0.0263,  0.0049,  0.0994, -0.1024],\n",
              "          [ 0.0876, -0.0009, -0.0502, -0.0319,  0.0045],\n",
              "          [-0.0527,  0.0894,  0.0727,  0.0599, -0.0293]]],\n",
              "\n",
              "\n",
              "        [[[ 0.0661, -0.0577, -0.0271, -0.0208,  0.0565],\n",
              "          [-0.0780,  0.0366, -0.0481, -0.0095,  0.0888],\n",
              "          [ 0.0366,  0.0029, -0.0905,  0.0884, -0.0130],\n",
              "          [-0.0337,  0.0671, -0.0660, -0.0564,  0.0002],\n",
              "          [-0.0624, -0.0889,  0.0728,  0.0057,  0.0076]],\n",
              "\n",
              "         [[ 0.0550, -0.0179, -0.0717, -0.0459, -0.0196],\n",
              "          [-0.0319, -0.0146, -0.0075, -0.0316, -0.0213],\n",
              "          [-0.0769, -0.0809, -0.0248,  0.0515, -0.0212],\n",
              "          [-0.0028, -0.0976,  0.0679,  0.0593, -0.0347],\n",
              "          [ 0.0093,  0.0901,  0.0795, -0.0656,  0.0904]],\n",
              "\n",
              "         [[ 0.0642,  0.0523, -0.0552,  0.0999,  0.0928],\n",
              "          [-0.0648, -0.0771,  0.0691,  0.0998,  0.0036],\n",
              "          [ 0.0218,  0.0376, -0.0017,  0.0812,  0.0758],\n",
              "          [-0.0441,  0.0303, -0.0959,  0.0481, -0.0279],\n",
              "          [-0.0475,  0.0297, -0.0260, -0.0429,  0.0165]],\n",
              "\n",
              "         [[ 0.0401, -0.0971,  0.0900,  0.0923, -0.0134],\n",
              "          [-0.0764, -0.0961,  0.0322, -0.0466,  0.0489],\n",
              "          [ 0.0108, -0.0184, -0.0364, -0.0680,  0.0861],\n",
              "          [-0.0883,  0.0216, -0.0736,  0.0348,  0.0445],\n",
              "          [ 0.0658,  0.0487, -0.0596, -0.0907,  0.0180]],\n",
              "\n",
              "         [[ 0.0009,  0.0365, -0.0627,  0.0014,  0.0308],\n",
              "          [-0.0059,  0.0272,  0.0044, -0.0040, -0.0171],\n",
              "          [-0.0932,  0.0379, -0.0814, -0.0226,  0.0123],\n",
              "          [ 0.0697,  0.0161,  0.0957,  0.0032,  0.0417],\n",
              "          [ 0.0014, -0.0848,  0.0274, -0.0299, -0.0503]],\n",
              "\n",
              "         [[-0.0317,  0.0621, -0.0144, -0.0878, -0.0425],\n",
              "          [ 0.0914, -0.0664,  0.0467, -0.0408,  0.0947],\n",
              "          [-0.0828, -0.0235,  0.0179, -0.0943,  0.0683],\n",
              "          [ 0.0370,  0.0004, -0.0483,  0.0903, -0.0417],\n",
              "          [ 0.0240,  0.0172,  0.0746, -0.0962, -0.0360]]],\n",
              "\n",
              "\n",
              "        ...,\n",
              "\n",
              "\n",
              "        [[[ 0.0277, -0.0695, -0.0978, -0.0440,  0.0014],\n",
              "          [ 0.0258,  0.0953, -0.0611,  0.0553,  0.0894],\n",
              "          [-0.0346, -0.0524,  0.0827, -0.0832,  0.0737],\n",
              "          [ 0.0317, -0.0070, -0.0063,  0.0821, -0.0321],\n",
              "          [-0.0264,  0.0375, -0.0739,  0.0798, -0.0695]],\n",
              "\n",
              "         [[ 0.0935,  0.0825,  0.0454,  0.1025, -0.0582],\n",
              "          [ 0.0446,  0.0221,  0.0022, -0.0120,  0.1031],\n",
              "          [-0.0189, -0.0357,  0.0307,  0.0607, -0.0799],\n",
              "          [ 0.0512,  0.0146, -0.0063, -0.0425,  0.0513],\n",
              "          [ 0.0189, -0.0719, -0.0236,  0.0303, -0.0074]],\n",
              "\n",
              "         [[ 0.0551,  0.0804,  0.0432,  0.0979,  0.0087],\n",
              "          [-0.1033, -0.0919, -0.0541,  0.0063,  0.0294],\n",
              "          [-0.0314, -0.0842,  0.0584,  0.0894, -0.0717],\n",
              "          [-0.0659, -0.0723,  0.1043,  0.0572,  0.0860],\n",
              "          [ 0.1009,  0.0446, -0.0941,  0.0064, -0.0608]],\n",
              "\n",
              "         [[ 0.0236, -0.0908, -0.0318, -0.0803,  0.0522],\n",
              "          [ 0.0744, -0.0786,  0.0798,  0.0602, -0.0238],\n",
              "          [ 0.0603,  0.0940,  0.1033,  0.0092, -0.0086],\n",
              "          [-0.0331,  0.1028,  0.0134,  0.0279, -0.0860],\n",
              "          [ 0.0589, -0.0719, -0.0352,  0.0289, -0.0988]],\n",
              "\n",
              "         [[ 0.0335,  0.0004,  0.0207, -0.0152,  0.0769],\n",
              "          [-0.0405, -0.0367, -0.0311, -0.0090, -0.0824],\n",
              "          [-0.0157, -0.0307,  0.0013,  0.0696, -0.0909],\n",
              "          [ 0.0903, -0.0910,  0.0214,  0.0277, -0.0561],\n",
              "          [-0.0843, -0.0433, -0.0480,  0.0449, -0.0722]],\n",
              "\n",
              "         [[-0.0688, -0.0737, -0.0826, -0.0218, -0.0104],\n",
              "          [ 0.0151,  0.0851, -0.0835,  0.0481, -0.0695],\n",
              "          [ 0.0291, -0.0235, -0.0376, -0.0922,  0.0165],\n",
              "          [ 0.0603, -0.0751,  0.0606, -0.0644,  0.0834],\n",
              "          [-0.0072, -0.0366,  0.0556, -0.0838,  0.0028]]],\n",
              "\n",
              "\n",
              "        [[[-0.0580,  0.0414,  0.0398, -0.0053, -0.0253],\n",
              "          [-0.0779, -0.0984,  0.0811,  0.0508,  0.0932],\n",
              "          [ 0.0292,  0.0032,  0.0562, -0.0663,  0.0157],\n",
              "          [ 0.0420, -0.0529, -0.0217, -0.0292, -0.0878],\n",
              "          [-0.0672,  0.0633, -0.0515,  0.0783, -0.0371]],\n",
              "\n",
              "         [[ 0.0074,  0.0832,  0.0491,  0.0588, -0.0638],\n",
              "          [-0.1017, -0.0855,  0.0691, -0.0331,  0.0790],\n",
              "          [-0.0330,  0.0907, -0.0444,  0.0862, -0.0563],\n",
              "          [ 0.0787, -0.0294,  0.0853,  0.0623,  0.0474],\n",
              "          [-0.0060, -0.0105, -0.0189,  0.0295,  0.0394]],\n",
              "\n",
              "         [[ 0.0124, -0.0534,  0.0451, -0.0886, -0.0316],\n",
              "          [ 0.0487, -0.0036, -0.0169,  0.0489,  0.0026],\n",
              "          [ 0.0961,  0.0718,  0.0943,  0.0159,  0.0049],\n",
              "          [ 0.0038,  0.0475,  0.0356,  0.0401, -0.0042],\n",
              "          [ 0.0699,  0.0857,  0.0812, -0.0599,  0.0568]],\n",
              "\n",
              "         [[ 0.0574,  0.0057, -0.0300, -0.1039,  0.0179],\n",
              "          [ 0.0141,  0.0645, -0.0484,  0.0226, -0.0133],\n",
              "          [-0.0298, -0.0275, -0.0857, -0.0344,  0.0212],\n",
              "          [-0.1024, -0.0022,  0.0564,  0.0437, -0.0684],\n",
              "          [-0.0017, -0.0954, -0.0636, -0.0982,  0.0971]],\n",
              "\n",
              "         [[-0.0763, -0.0297,  0.0301,  0.0575,  0.0138],\n",
              "          [ 0.0173, -0.0789,  0.0649, -0.0956, -0.0508],\n",
              "          [-0.0099, -0.0167,  0.0533, -0.0003,  0.0498],\n",
              "          [-0.0291,  0.0250, -0.0072,  0.0826, -0.0850],\n",
              "          [-0.0022,  0.0233, -0.0863,  0.0125,  0.0235]],\n",
              "\n",
              "         [[ 0.0425,  0.0977, -0.0519,  0.0302,  0.0721],\n",
              "          [-0.0891,  0.0381,  0.0002, -0.0061,  0.0541],\n",
              "          [ 0.0842,  0.0849, -0.0301, -0.0223,  0.0708],\n",
              "          [ 0.0029, -0.0627, -0.0306,  0.0103,  0.0847],\n",
              "          [-0.0313, -0.0055, -0.0111,  0.0153, -0.0691]]],\n",
              "\n",
              "\n",
              "        [[[-0.0658,  0.0307,  0.0520,  0.0786, -0.0633],\n",
              "          [-0.1043, -0.0382,  0.1024, -0.0986,  0.0667],\n",
              "          [-0.0993,  0.0368,  0.0867, -0.0028, -0.0620],\n",
              "          [ 0.0936,  0.0302,  0.0366,  0.0802, -0.0551],\n",
              "          [-0.0005, -0.0833, -0.0902,  0.0157,  0.0884]],\n",
              "\n",
              "         [[-0.0530,  0.0915,  0.0124, -0.0052,  0.1028],\n",
              "          [-0.0713,  0.0725,  0.0517, -0.0372,  0.0947],\n",
              "          [ 0.0071,  0.0518, -0.0585, -0.0997, -0.0550],\n",
              "          [ 0.0571, -0.0062, -0.0272,  0.0625, -0.0979],\n",
              "          [-0.0411,  0.0758,  0.0729,  0.0198,  0.0504]],\n",
              "\n",
              "         [[ 0.0505,  0.0486, -0.0801,  0.0050, -0.0130],\n",
              "          [-0.0807,  0.0666, -0.0237,  0.0956, -0.0752],\n",
              "          [-0.0353,  0.0176,  0.0243,  0.0743, -0.0673],\n",
              "          [-0.0755,  0.0112,  0.0328,  0.0219, -0.0905],\n",
              "          [ 0.0598,  0.1038, -0.0110, -0.0080, -0.0711]],\n",
              "\n",
              "         [[ 0.0814, -0.0605, -0.0808, -0.0353, -0.0488],\n",
              "          [ 0.0087,  0.0533,  0.0822, -0.0395, -0.0033],\n",
              "          [-0.0875, -0.0879, -0.0484,  0.1021, -0.0776],\n",
              "          [-0.0008,  0.0015,  0.0116,  0.0747, -0.0045],\n",
              "          [-0.0676, -0.0346, -0.0846,  0.0030, -0.0618]],\n",
              "\n",
              "         [[ 0.0275,  0.0007, -0.0729,  0.0811, -0.0561],\n",
              "          [ 0.0963,  0.0315, -0.0446,  0.0679, -0.0952],\n",
              "          [ 0.0598,  0.0154, -0.0114,  0.0641,  0.0947],\n",
              "          [ 0.0619,  0.0882,  0.0564,  0.0271, -0.0217],\n",
              "          [ 0.0454, -0.0861, -0.0870, -0.0979,  0.0483]],\n",
              "\n",
              "         [[-0.0367,  0.0650, -0.0905, -0.0508,  0.0051],\n",
              "          [-0.0906, -0.0155, -0.0440, -0.0907, -0.0218],\n",
              "          [-0.0162,  0.0054,  0.0963, -0.0341, -0.0043],\n",
              "          [-0.0183,  0.0726,  0.0828,  0.1012, -0.0405],\n",
              "          [-0.0885, -0.0178, -0.1009, -0.0912,  0.0224]]]], device='cuda:0',\n",
              "       requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 313
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "from torchvision.datasets import CIFAR10\n",
        "train_transforms = transforms.Compose([ transforms.ToTensor()])\n",
        "dataset = CIFAR10(root=\"./test/\", train=False, download=True, transform=train_transforms)\n",
        "\n",
        "\n",
        "train_set, test_set = torch.utils.data.random_split(dataset, [8000, 2000])\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader( train_set, batch_size=1024,shuffle=True)\n",
        "testloader = torch.utils.data.DataLoader( test_set,batch_size=1024, shuffle=True)"
      ],
      "metadata": {
        "id": "CRKMJj0BI36u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bb79057-e378-412a-8ae1-9fc0b2674cf1"
      },
      "execution_count": 314,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.LBFGS(model.parameters(),lr=0.35,max_iter=4,history_size=10,line_search_fn='strong_wolfe')\n",
        "\n"
      ],
      "metadata": {
        "id": "GW0vb9vmIQ63"
      },
      "execution_count": 315,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N_EPOCHS = 50\n",
        "for epoch in range(N_EPOCHS):\n",
        "  epoch_loss = 0.0\n",
        "  i = 0\n",
        "  model.train()\n",
        "  for inputs, labels in trainloader:\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    def closure():\n",
        "      optimizer.zero_grad()\n",
        "      outputs = model(inputs)\n",
        "      loss = criterion(outputs, labels)\n",
        "      loss.backward(retain_graph=True)\n",
        "\n",
        "      return loss\n",
        "\n",
        "    optimizer.step(closure)\n",
        "\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "    epoch_loss += loss.item()\n",
        "\n",
        "  val_loss = 0.0\n",
        "  model.eval()\n",
        "  for inputs, labels in testloader:\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "    val_loss += loss.item()\n",
        "  print(\"Epoch: {} Train Loss: {} Val Loss: {} \".format(epoch, epoch_loss/len(trainloader), val_loss/len(testloader)))\n",
        "\n"
      ],
      "metadata": {
        "id": "45lkIbuzIVNE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0490704d-7c2b-4130-ce67-3ca7a3cd7135"
      },
      "execution_count": 316,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 Train Loss: 2.302582234144211 Val Loss: 2.302242159843445 \n",
            "Epoch: 1 Train Loss: 2.302254170179367 Val Loss: 2.302234172821045 \n",
            "Epoch: 2 Train Loss: 2.301358014345169 Val Loss: 2.3020946979522705 \n",
            "Epoch: 3 Train Loss: 2.302258551120758 Val Loss: 2.3020176887512207 \n",
            "Epoch: 4 Train Loss: 2.300609827041626 Val Loss: 2.298862934112549 \n",
            "Epoch: 5 Train Loss: 2.199757754802704 Val Loss: 2.0873111486434937 \n",
            "Epoch: 6 Train Loss: 2.050920605659485 Val Loss: 2.0390113592147827 \n",
            "Epoch: 7 Train Loss: 2.0329954624176025 Val Loss: 2.0269094705581665 \n",
            "Epoch: 8 Train Loss: 2.0245636105537415 Val Loss: 2.0117079615592957 \n",
            "Epoch: 9 Train Loss: 2.013158768415451 Val Loss: 2.009145140647888 \n",
            "Epoch: 10 Train Loss: 1.9904993027448654 Val Loss: 2.0010342597961426 \n",
            "Epoch: 11 Train Loss: 1.9889756739139557 Val Loss: 1.9964798092842102 \n",
            "Epoch: 12 Train Loss: 1.9787068963050842 Val Loss: 1.98788321018219 \n",
            "Epoch: 13 Train Loss: 1.950659066438675 Val Loss: 1.964683711528778 \n",
            "Epoch: 14 Train Loss: 1.9169347435235977 Val Loss: 1.946733832359314 \n",
            "Epoch: 15 Train Loss: 1.9019780457019806 Val Loss: 1.9354785084724426 \n",
            "Epoch: 16 Train Loss: 1.8955067843198776 Val Loss: 1.9523118138313293 \n",
            "Epoch: 17 Train Loss: 1.8751225769519806 Val Loss: 1.8846789598464966 \n",
            "Epoch: 18 Train Loss: 1.8452272415161133 Val Loss: 1.8477483987808228 \n",
            "Epoch: 19 Train Loss: 1.836418554186821 Val Loss: 1.8536441922187805 \n",
            "Epoch: 20 Train Loss: 1.8386141955852509 Val Loss: 1.8549879789352417 \n",
            "Epoch: 21 Train Loss: 1.8236355632543564 Val Loss: 1.8245511651039124 \n",
            "Epoch: 22 Train Loss: 1.816511407494545 Val Loss: 1.8226606249809265 \n",
            "Epoch: 23 Train Loss: 1.8091261684894562 Val Loss: 1.810290515422821 \n",
            "Epoch: 24 Train Loss: 1.7869967967271805 Val Loss: 1.7844349145889282 \n",
            "Epoch: 25 Train Loss: 1.7738809436559677 Val Loss: 1.7568951845169067 \n",
            "Epoch: 26 Train Loss: 1.748584121465683 Val Loss: 1.7672719955444336 \n",
            "Epoch: 27 Train Loss: 1.7229553759098053 Val Loss: 1.7451320886611938 \n",
            "Epoch: 28 Train Loss: 1.7202002555131912 Val Loss: 1.7360588312149048 \n",
            "Epoch: 29 Train Loss: 1.7177785336971283 Val Loss: 1.7364688515663147 \n",
            "Epoch: 30 Train Loss: 1.720703512430191 Val Loss: 1.730507493019104 \n",
            "Epoch: 31 Train Loss: 1.7203092128038406 Val Loss: 1.7280411124229431 \n",
            "Epoch: 32 Train Loss: 1.7185734808444977 Val Loss: 1.7222878336906433 \n",
            "Epoch: 33 Train Loss: 1.7083315402269363 Val Loss: 1.6929103136062622 \n",
            "Epoch: 34 Train Loss: 1.6963270455598831 Val Loss: 1.68900728225708 \n",
            "Epoch: 35 Train Loss: 1.6740540266036987 Val Loss: 1.6597375869750977 \n",
            "Epoch: 36 Train Loss: 1.6480636894702911 Val Loss: 1.6385226845741272 \n",
            "Epoch: 37 Train Loss: 1.6532749086618423 Val Loss: 1.638000726699829 \n",
            "Epoch: 38 Train Loss: 1.6446952670812607 Val Loss: 1.6290662288665771 \n",
            "Epoch: 39 Train Loss: 1.641973152756691 Val Loss: 1.629617989063263 \n",
            "Epoch: 40 Train Loss: 1.649159386754036 Val Loss: 1.6283523440361023 \n",
            "Epoch: 41 Train Loss: 1.643138512969017 Val Loss: 1.6269420385360718 \n",
            "Epoch: 42 Train Loss: 1.6436227411031723 Val Loss: 1.6279066801071167 \n",
            "Epoch: 43 Train Loss: 1.6515580266714096 Val Loss: 1.627452552318573 \n",
            "Epoch: 44 Train Loss: 1.6439211070537567 Val Loss: 1.6273890733718872 \n",
            "Epoch: 45 Train Loss: 1.6469082683324814 Val Loss: 1.6273292303085327 \n",
            "Epoch: 46 Train Loss: 1.640976443886757 Val Loss: 1.6274510622024536 \n",
            "Epoch: 47 Train Loss: 1.6441877484321594 Val Loss: 1.6273802518844604 \n",
            "Epoch: 48 Train Loss: 1.6364803612232208 Val Loss: 1.6167445182800293 \n",
            "Epoch: 49 Train Loss: 1.6364972293376923 Val Loss: 1.614358127117157 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model(test_set[901][0].unsqueeze(0).to(device))\n",
        "print(f\" Model predicted label : {np.argmax(y_pred.detach().cpu().numpy())}\")\n",
        "print(f\" Actual label : {test_set[901][1]}\")"
      ],
      "metadata": {
        "id": "Bz8h1KFp0dWn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "034a6ada-8186-4829-9088-06957ba19f2d"
      },
      "execution_count": 320,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Model predicted label : 1\n",
            " Actual label : 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model(train_set[9][0].unsqueeze(0).to(device))\n",
        "print(f\" Model predicted label : {np.argmax(y_pred.detach().cpu().numpy())}\")\n",
        "print(f\" Actual label : {train_set[9][1]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5RFHBBMYoZI",
        "outputId": "4b12b981-fb43-49f5-eebf-4f900fa15480"
      },
      "execution_count": 318,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Model predicted label : 1\n",
            " Actual label : 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wp4MPQlQzK5O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}