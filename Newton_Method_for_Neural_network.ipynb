{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPH9chyZ+NB8UwqPHBYu0Hc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahulsm27/ML/blob/main/Newton_Method_for_Neural_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SzfDHOW9ID9I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9752c502-b8b8-40d6-bc56-87cf7ab10a0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-minimize\n",
            "  Downloading pytorch_minimize-0.0.2-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-minimize) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.6 in /usr/local/lib/python3.10/dist-packages (from pytorch-minimize) (1.11.3)\n",
            "Requirement already satisfied: torch>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-minimize) (2.1.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->pytorch-minimize) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->pytorch-minimize) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->pytorch-minimize) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->pytorch-minimize) (3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->pytorch-minimize) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->pytorch-minimize) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->pytorch-minimize) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.9.0->pytorch-minimize) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.9.0->pytorch-minimize) (1.3.0)\n",
            "Installing collected packages: pytorch-minimize\n",
            "Successfully installed pytorch-minimize-0.0.2\n"
          ]
        }
      ],
      "source": [
        "# # Installing required libraries\n",
        "# !pip install torchinfo\n",
        "# import torchinfo\n",
        "!pip install pytorch-minimize\n",
        "import torch\n",
        "from torch import nn,optim\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "#device = \"cpu\""
      ],
      "metadata": {
        "id": "V6P_7CeWIGlz"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating our own LeNet5\n",
        "\n",
        "class LeNet5(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(LeNet5,self).__init__()\n",
        "    self.conv1 = nn.Conv2d(3,6,5) # in channel , out channe, kernel\n",
        "    self.relu1 = nn.ReLU()\n",
        "    self.maxpool1 = nn.MaxPool2d((2,2))\n",
        "\n",
        "    self.conv2 = nn.Conv2d(6,16,5)  # in channel , out channe, kernel\n",
        "    self.relu2 = nn.ReLU()\n",
        "    self.maxpool2 = nn.MaxPool2d((2,2))\n",
        "\n",
        "    self.fc1 = nn.Linear(16*5*5,120)\n",
        "    self.fc2 = nn.Linear(120,84)\n",
        "    self.fc3 = nn.Linear(84,10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "\n",
        "    x = self.relu1(x)\n",
        "    x= self.maxpool1(x)\n",
        "\n",
        "    x = self.conv2(x)\n",
        "    x = self.relu2(x)\n",
        "    x= self.maxpool2(x)\n",
        "\n",
        "    x = x.view(-1,int(x.nelement() / x.shape[0]))\n",
        "\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "\n",
        "\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "model = LeNet5().to(device)\n",
        "torch.nn.init.xavier_uniform_(model.conv1.weight)\n",
        "torch.nn.init.xavier_uniform_(model.conv2.weight)\n",
        "torch.nn.init.xavier_uniform_(model.fc1.weight)\n",
        "torch.nn.init.xavier_uniform_(model.fc2.weight)\n",
        "torch.nn.init.xavier_uniform_(model.fc3.weight)\n",
        "\n"
      ],
      "metadata": {
        "id": "LM9gGCNQIODF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f08c515a-5d32-4efd-e051-9a9d947a18cd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[-1.6266e-01,  1.4402e-01, -2.4872e-01, -2.3892e-01,  5.8217e-02,\n",
              "         -1.2274e-01, -1.5620e-01,  1.4433e-01,  7.7499e-02, -1.5581e-01,\n",
              "          1.2261e-01,  1.5671e-01,  1.7872e-01,  2.4279e-01, -8.7386e-02,\n",
              "          8.9809e-02, -1.1898e-01, -6.8837e-02, -2.5565e-03,  9.2045e-02,\n",
              "         -1.8321e-01, -4.7986e-02,  1.5248e-02,  1.0698e-01,  1.9954e-01,\n",
              "         -1.1392e-01,  1.6589e-02, -2.0039e-01,  2.0894e-01, -5.5351e-02,\n",
              "         -1.0115e-01,  8.2095e-02,  3.0908e-02, -2.2902e-01,  9.2432e-02,\n",
              "          9.8669e-02, -2.4258e-01,  8.3978e-03, -2.2438e-01, -1.2905e-02,\n",
              "          2.4736e-02,  2.3419e-01,  1.9899e-01, -2.1639e-01,  8.5183e-02,\n",
              "         -3.0988e-02,  1.9282e-01, -2.4545e-01,  2.4043e-01, -1.4035e-01,\n",
              "          1.8520e-01, -2.0123e-01,  1.6688e-02,  4.1007e-02,  8.7395e-02,\n",
              "         -2.4793e-01,  4.3835e-02,  4.7252e-02,  7.7107e-02, -1.7631e-01,\n",
              "         -1.6404e-01, -2.4897e-01, -4.4764e-02, -8.4548e-02,  1.5567e-01,\n",
              "          9.3926e-03, -9.9965e-02,  1.9251e-01, -2.0846e-01,  1.8602e-01,\n",
              "          1.4082e-01, -9.0460e-02, -8.0225e-02,  1.4494e-01, -8.7317e-02,\n",
              "         -2.1027e-01,  1.2748e-01,  1.7014e-02, -1.0138e-01,  8.3726e-03,\n",
              "          2.2926e-01, -1.3246e-01, -1.3998e-01, -5.4509e-02],\n",
              "        [ 3.6360e-02, -2.1588e-01,  1.8174e-01, -1.6161e-01,  6.2147e-02,\n",
              "         -3.7439e-02,  1.3069e-01, -2.0656e-01, -5.9168e-02, -1.3075e-01,\n",
              "          1.4096e-02, -1.6131e-01, -8.6401e-02, -1.2264e-01, -8.0242e-02,\n",
              "          2.1336e-01,  1.6818e-01, -2.2508e-02,  1.8661e-01, -7.7908e-02,\n",
              "         -3.0901e-02,  1.8214e-01, -1.3023e-01, -1.5189e-01, -1.3053e-01,\n",
              "          2.2842e-01, -4.9454e-02,  7.2771e-02,  2.3792e-01,  1.6855e-01,\n",
              "          1.5954e-01, -3.7300e-02, -1.0042e-01,  1.9623e-01, -3.2054e-02,\n",
              "          5.0958e-02,  4.4052e-02,  1.8768e-01, -1.2682e-01, -4.9246e-02,\n",
              "          2.8534e-02, -2.1191e-01, -2.1731e-03,  1.5656e-01, -1.5404e-01,\n",
              "          7.6304e-02, -8.1348e-02, -2.2161e-01,  6.8541e-02,  2.2357e-01,\n",
              "          1.5586e-01,  5.5296e-02, -2.7853e-02, -1.7704e-01, -3.4451e-02,\n",
              "         -1.2010e-01,  2.0569e-01,  1.5801e-01,  1.9364e-01,  1.6285e-01,\n",
              "         -4.8197e-02, -2.1155e-01, -7.3147e-02,  1.0524e-01,  1.5704e-01,\n",
              "          2.1029e-01,  1.8167e-01, -1.1969e-01, -1.6884e-01, -1.6007e-01,\n",
              "         -2.2508e-01,  1.1261e-01, -2.4619e-02, -1.2076e-01, -1.5438e-01,\n",
              "          1.3074e-01, -3.1145e-02, -1.3140e-01,  9.4979e-02,  4.6918e-03,\n",
              "          6.8610e-02, -1.9570e-01,  9.9716e-02, -1.0015e-01],\n",
              "        [-2.4519e-01, -1.1077e-01, -6.2912e-02,  1.2722e-01, -2.4418e-01,\n",
              "          1.5223e-01,  9.3755e-02, -8.4120e-02,  8.3923e-03, -9.9245e-02,\n",
              "          1.8505e-01,  4.7493e-02, -4.7536e-03,  3.2274e-02, -1.4762e-01,\n",
              "          1.8836e-01, -1.6174e-01, -2.3233e-01, -6.6784e-02,  2.3265e-01,\n",
              "         -9.9571e-02, -4.4894e-02, -2.5268e-02, -2.2757e-01,  1.6073e-02,\n",
              "         -1.4637e-01, -3.8837e-03, -2.3258e-01,  2.7831e-02,  4.6141e-02,\n",
              "          1.0188e-01,  1.7776e-01,  2.0721e-01,  2.3418e-01, -1.1387e-01,\n",
              "         -1.6930e-01,  2.3639e-01,  7.0259e-02, -3.2936e-03,  1.5835e-01,\n",
              "          1.4865e-01, -1.6864e-01,  1.8308e-02, -5.5885e-02, -7.7219e-02,\n",
              "         -1.6070e-01, -5.0803e-02, -6.8775e-03, -2.1461e-01, -1.2549e-01,\n",
              "          1.3192e-01, -2.3603e-01,  1.9740e-01, -2.0225e-01,  1.9450e-01,\n",
              "          1.8449e-01,  8.7040e-02,  1.3162e-01,  6.4672e-02, -3.5220e-02,\n",
              "          1.8387e-01, -2.2190e-03, -3.2495e-02, -9.7092e-02,  1.0732e-01,\n",
              "         -2.0856e-03, -1.7867e-01, -2.1510e-01, -7.8156e-02,  2.4904e-01,\n",
              "         -2.3001e-01,  1.7529e-01,  6.7448e-02, -2.1868e-01, -1.7025e-02,\n",
              "         -1.5038e-01, -1.5650e-01,  1.8167e-02,  4.8011e-02, -5.1704e-02,\n",
              "          1.7793e-01,  1.6443e-02, -8.7122e-02, -1.2182e-01],\n",
              "        [-1.6988e-01, -1.9221e-01,  1.7266e-01,  2.4363e-01, -5.6702e-02,\n",
              "          2.2620e-01,  1.8949e-01,  7.7513e-02, -6.5034e-02, -2.3556e-01,\n",
              "          1.6308e-01,  1.5445e-02,  1.1911e-01, -1.2136e-02,  1.3179e-01,\n",
              "          1.7642e-01,  1.2337e-01, -1.0741e-01, -2.1039e-01,  1.5621e-01,\n",
              "         -1.9807e-01,  2.4431e-01,  1.3843e-01, -1.6094e-01,  1.2664e-01,\n",
              "         -1.0686e-02,  2.1732e-02, -2.0378e-01,  1.7328e-01, -2.1664e-01,\n",
              "         -3.1515e-02, -1.6137e-01, -1.9278e-01, -1.8577e-01,  1.0852e-01,\n",
              "          2.2397e-01, -6.1993e-02,  2.3463e-01, -1.3530e-01,  1.3534e-02,\n",
              "         -3.6614e-02,  1.6310e-02,  1.8085e-01, -1.3082e-01,  1.9455e-01,\n",
              "         -4.6814e-02,  6.5789e-02,  1.4833e-01, -2.1061e-01,  2.5145e-01,\n",
              "          5.2536e-02, -1.6736e-01,  2.2714e-01, -1.6613e-01, -9.9261e-02,\n",
              "          2.5052e-01,  1.0015e-01,  1.9390e-01, -2.2317e-01,  8.0305e-02,\n",
              "         -8.0881e-02,  1.3794e-01, -2.3267e-01, -2.2176e-01, -7.3547e-05,\n",
              "         -2.4532e-01,  6.8630e-02,  6.1279e-02, -1.9538e-02, -1.3355e-02,\n",
              "          1.8526e-01,  2.4925e-01,  1.7027e-01, -8.8112e-02,  2.1591e-01,\n",
              "          1.8984e-01,  2.3583e-01,  1.0892e-01,  1.6653e-01,  9.3908e-02,\n",
              "          2.4412e-01, -1.8589e-02, -4.8707e-02, -6.9666e-02],\n",
              "        [-1.5413e-01,  2.3642e-01,  1.7922e-01, -3.2024e-02,  2.0905e-01,\n",
              "         -8.4033e-03,  1.4362e-01, -1.6926e-01, -1.0642e-02,  5.2995e-02,\n",
              "         -1.6626e-01, -3.1437e-02,  8.0185e-02,  1.9732e-01, -2.1033e-01,\n",
              "         -2.7579e-02,  1.0698e-01,  1.8610e-01, -2.4456e-01,  3.5432e-02,\n",
              "          1.4129e-01, -6.5399e-02,  1.4247e-01, -1.6836e-01, -1.3055e-01,\n",
              "          2.5200e-01,  1.5434e-01,  2.3632e-01, -3.7425e-02,  7.2414e-02,\n",
              "         -2.4422e-01, -5.5457e-02,  2.2499e-01,  8.4818e-02, -1.1029e-01,\n",
              "          7.0300e-02, -2.4605e-01, -7.9406e-02, -2.4922e-01,  1.9038e-01,\n",
              "         -1.0108e-01,  6.1064e-02,  1.8230e-01,  1.0048e-01, -2.1362e-01,\n",
              "         -1.7972e-02, -2.0028e-02, -3.7608e-02, -1.2387e-01, -4.4742e-02,\n",
              "          1.9890e-01, -1.1727e-01,  1.5326e-01, -8.3097e-02, -3.1713e-02,\n",
              "          4.8061e-02,  1.1351e-01, -1.3483e-02,  2.3213e-01,  1.8586e-01,\n",
              "         -2.5714e-02, -9.7922e-02, -2.0934e-01,  2.5035e-01,  7.4492e-02,\n",
              "          2.5110e-01,  1.7571e-01, -3.0608e-02, -1.1032e-01, -1.1084e-01,\n",
              "         -1.3932e-01, -1.3833e-03,  1.9983e-01, -1.7341e-01,  6.6774e-03,\n",
              "         -1.1518e-01, -5.6052e-02,  1.6551e-01, -1.7807e-01,  4.3517e-02,\n",
              "         -1.0372e-01,  9.7938e-02,  2.0569e-01, -4.6647e-02],\n",
              "        [ 1.3880e-01,  1.8935e-01, -1.8144e-01,  8.6524e-02,  1.4416e-01,\n",
              "          8.7802e-02, -1.5337e-01,  2.3834e-01, -2.3991e-01, -1.6997e-01,\n",
              "          2.1968e-01, -2.1811e-02, -3.3564e-02, -4.4289e-03, -1.1284e-01,\n",
              "         -2.2524e-01, -2.3603e-01, -3.1249e-02,  2.1822e-01,  1.2981e-01,\n",
              "          3.2668e-02, -1.5765e-01,  5.8443e-02,  1.5812e-01,  3.1060e-02,\n",
              "         -2.0077e-01, -1.8081e-01,  2.2270e-01, -1.7171e-01,  5.3455e-02,\n",
              "         -1.6005e-01,  1.2760e-01, -1.4402e-01,  1.7571e-01,  1.0401e-01,\n",
              "         -2.0538e-01, -2.3599e-01,  1.0481e-01, -1.8661e-01,  1.3046e-01,\n",
              "         -2.0397e-01, -4.2161e-03, -2.0932e-01,  2.2147e-01, -1.6456e-01,\n",
              "         -4.0427e-02,  9.1820e-02,  2.1248e-01, -1.5007e-01,  4.7653e-03,\n",
              "         -2.0044e-01, -2.4504e-01, -7.9720e-03,  2.2689e-02,  1.3126e-01,\n",
              "          7.1570e-02, -7.3880e-02,  8.8292e-02, -4.2897e-02, -9.6590e-02,\n",
              "          1.1057e-01, -2.0911e-01, -1.9805e-01, -1.2030e-01,  7.7184e-02,\n",
              "         -2.0919e-01,  2.2374e-01,  1.0266e-01, -3.2107e-02, -1.9440e-01,\n",
              "          2.3769e-01, -1.1603e-01,  2.4243e-01,  7.4799e-02, -4.1842e-02,\n",
              "          6.2873e-02, -1.4507e-01, -1.0046e-01, -3.2750e-02, -1.7670e-01,\n",
              "         -1.0700e-01, -1.0651e-01,  2.1366e-01,  3.9626e-02],\n",
              "        [ 7.5948e-03, -2.4717e-01, -9.4989e-02,  1.7141e-01,  3.3007e-02,\n",
              "          6.5565e-02, -1.0398e-01,  6.5957e-02,  4.4662e-02, -7.4372e-02,\n",
              "          9.0001e-02,  2.5180e-01,  1.2200e-01, -1.3286e-01, -2.2761e-01,\n",
              "          2.3305e-01,  8.0381e-02, -8.5100e-02,  2.3532e-01,  5.3078e-03,\n",
              "         -2.1224e-01,  1.9918e-01, -4.8442e-02,  5.3997e-02,  7.2804e-03,\n",
              "         -1.9262e-01, -1.5952e-01,  2.3769e-01, -3.6593e-02,  1.7316e-01,\n",
              "         -3.7325e-02,  1.8753e-01,  1.2337e-01,  2.1829e-01, -1.6500e-01,\n",
              "         -8.7743e-02,  1.8117e-01, -2.3753e-01,  8.5532e-03,  1.3902e-02,\n",
              "          2.6137e-02, -1.2546e-01,  2.3041e-01,  3.0032e-02,  1.5038e-01,\n",
              "          1.6431e-01, -2.4924e-01, -1.5839e-01,  1.0082e-01,  1.6365e-01,\n",
              "         -1.1219e-02,  2.2485e-02, -1.0385e-01, -8.2863e-02, -5.0999e-02,\n",
              "          1.2109e-01,  6.8728e-02, -1.7214e-01, -8.9930e-02, -6.1385e-02,\n",
              "          1.3513e-01,  1.9552e-01, -1.3180e-01, -1.6254e-01, -1.1658e-01,\n",
              "          1.8536e-01, -1.6922e-02, -1.9211e-01,  1.8680e-01,  2.6313e-02,\n",
              "          8.7471e-02, -1.3966e-01, -3.4346e-02, -1.1741e-01,  1.2889e-01,\n",
              "         -1.0237e-01,  1.6272e-01, -9.2340e-02,  1.0022e-01, -1.4738e-01,\n",
              "          4.7157e-02,  1.4360e-01, -1.0635e-01, -7.0245e-02],\n",
              "        [ 1.7488e-01,  1.5205e-01,  1.9373e-01,  5.7644e-02, -2.0598e-02,\n",
              "          3.2151e-02, -1.8164e-01, -1.9700e-01,  8.4491e-02, -9.0606e-02,\n",
              "          2.4692e-01, -1.8016e-01, -1.7291e-01,  1.8107e-02,  4.1756e-02,\n",
              "         -1.8005e-01,  7.9727e-02,  2.0274e-01,  2.0091e-01, -1.5856e-02,\n",
              "          8.5620e-02, -2.3563e-01,  2.4896e-01,  1.1379e-01, -2.0090e-01,\n",
              "          1.5029e-01,  5.1025e-02, -2.0276e-01,  1.6720e-01,  3.5946e-02,\n",
              "         -1.9612e-01, -1.0428e-01, -1.4670e-01, -7.2493e-02,  1.1902e-01,\n",
              "         -1.2932e-01, -1.3722e-01,  1.2170e-01,  7.5177e-02,  1.1478e-01,\n",
              "         -2.0048e-01,  2.9171e-02,  1.5446e-01,  5.7513e-03,  1.1028e-01,\n",
              "         -8.6182e-02,  2.4998e-01,  1.8252e-01,  2.0303e-01,  1.0940e-01,\n",
              "          6.1897e-02, -1.7156e-02, -2.3330e-01,  1.6691e-01, -2.3067e-01,\n",
              "          2.1513e-01, -1.0770e-01,  1.0664e-01,  2.5203e-01,  9.9629e-02,\n",
              "         -1.9856e-02, -1.5635e-01,  1.1701e-01,  1.5717e-01,  1.9310e-01,\n",
              "          9.7684e-02,  3.1865e-02,  6.0389e-02, -6.0705e-02,  1.6625e-01,\n",
              "          1.9273e-01,  1.9873e-01, -2.9497e-02, -5.8845e-02,  1.2315e-01,\n",
              "         -1.7525e-01,  1.6897e-01,  6.3968e-02, -1.9632e-01, -1.6198e-01,\n",
              "         -1.9603e-01, -1.0657e-01,  1.1777e-01, -1.1388e-01],\n",
              "        [-2.3774e-01,  5.4970e-02,  1.2884e-01, -7.1233e-02, -2.3187e-01,\n",
              "         -4.2130e-02,  8.4862e-03,  2.5623e-02, -1.3730e-01,  6.1978e-02,\n",
              "         -1.4338e-01,  5.8644e-02, -2.5687e-02,  7.8204e-02, -4.2897e-02,\n",
              "         -7.1012e-02,  1.7356e-01,  3.0729e-02, -2.3682e-01, -2.4761e-02,\n",
              "          2.4921e-01, -2.1891e-01, -9.5009e-02, -1.6172e-02,  7.5044e-03,\n",
              "          1.5254e-01, -1.6051e-01, -9.5349e-02, -4.9320e-03, -5.8567e-03,\n",
              "         -1.8919e-01, -3.4917e-02,  1.6217e-01,  7.5424e-02, -2.4543e-01,\n",
              "         -3.2336e-02, -1.6064e-01,  1.6934e-02, -6.5813e-02,  1.5369e-01,\n",
              "         -4.6990e-03,  1.9573e-01, -3.6622e-02, -1.5528e-02,  5.9761e-02,\n",
              "         -7.8000e-02,  7.9932e-02, -2.0550e-01,  2.1135e-01, -7.7247e-02,\n",
              "         -3.3080e-02,  1.5760e-01,  1.0096e-01,  1.3346e-01, -2.2012e-02,\n",
              "         -1.9445e-01,  1.9268e-01, -3.0513e-02,  2.2030e-01,  1.6157e-01,\n",
              "          1.3575e-01, -9.4907e-02, -2.0390e-01,  2.2733e-01,  1.9368e-01,\n",
              "          1.4751e-01,  1.2528e-01, -7.3241e-03,  1.6726e-01, -1.8332e-01,\n",
              "          1.8037e-01, -5.5602e-02, -1.0273e-01,  8.1709e-02, -1.5976e-01,\n",
              "         -1.7674e-02,  1.9439e-01, -2.4597e-01,  2.3531e-01,  1.1064e-01,\n",
              "         -4.2736e-02, -1.0835e-02, -2.5020e-01,  1.2277e-01],\n",
              "        [-5.4071e-02, -1.6406e-01, -2.1637e-01, -1.2011e-01, -1.5458e-01,\n",
              "          1.0612e-01,  2.5005e-01, -4.9215e-02,  5.3407e-02, -2.0469e-01,\n",
              "          9.0102e-02,  3.8258e-02,  9.5524e-02,  1.9963e-01,  9.4773e-02,\n",
              "          1.4167e-01, -9.2669e-03, -2.3152e-01,  2.3881e-01,  1.1966e-01,\n",
              "          8.3728e-02,  5.5806e-02,  2.0530e-01,  1.3661e-01, -1.2798e-01,\n",
              "          8.7104e-02,  1.2314e-01,  1.1238e-01,  1.7381e-01,  2.0063e-01,\n",
              "         -1.2453e-01,  2.3819e-01, -1.4882e-01, -1.4730e-02, -2.3990e-01,\n",
              "         -7.2635e-02,  2.4243e-01,  1.3379e-01,  2.4394e-01, -2.1757e-01,\n",
              "          1.3984e-01,  6.2786e-03, -1.5474e-01,  1.9770e-01, -1.8837e-01,\n",
              "         -1.5769e-01, -2.4612e-01,  1.0770e-01,  8.6064e-02,  3.7798e-02,\n",
              "          5.1677e-02, -2.4521e-01,  2.1267e-01,  6.2445e-02,  1.1159e-01,\n",
              "         -3.1328e-02,  6.3529e-02,  9.3107e-02, -2.8489e-02, -3.0536e-02,\n",
              "         -3.1424e-02,  2.0560e-01, -2.2898e-01, -3.3646e-02, -2.0930e-01,\n",
              "          1.7908e-01,  1.4618e-01,  1.6618e-01, -6.6247e-02, -1.6377e-01,\n",
              "          1.6960e-01, -1.1827e-01, -2.6080e-02,  5.5713e-02, -1.6828e-01,\n",
              "          2.9777e-02,  9.1363e-02,  2.4478e-01,  3.6164e-02, -1.2380e-01,\n",
              "          8.0637e-02, -2.4336e-01, -2.0455e-01,  1.8388e-01]], device='cuda:0',\n",
              "       requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "from torchvision.datasets import CIFAR10\n",
        "train_transforms = transforms.Compose([ transforms.ToTensor(), transforms.Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010))])\n",
        "train_data = CIFAR10(root=\"./train/\", train=False, download=True, transform=train_transforms)\n",
        "trainloader = torch.utils.data.DataLoader( train_data, shuffle=True)"
      ],
      "metadata": {
        "id": "CRKMJj0BI36u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e7dcff2-dd90-475b-8bfc-86246143fb1f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./train/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:02<00:00, 79362730.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./train/cifar-10-python.tar.gz to ./train/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchmin\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torchmin.Minimizer(model.parameters(),method='l-bfgs')\n"
      ],
      "metadata": {
        "id": "GW0vb9vmIQ63"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N_EPOCHS = 5\n",
        "for epoch in range(N_EPOCHS):\n",
        "  epoch_loss = 0.0\n",
        "  for inputs, labels in trainloader:\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "\n",
        "    def closure():\n",
        "      optimizer.zero_grad()\n",
        "      outputs = model(inputs)\n",
        "      loss = criterion(outputs, labels)\n",
        "      loss.backward(retain_graph=True)\n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(),1.0e-2)\n",
        "\n",
        "      return loss\n",
        "    optimizer.step(closure)\n",
        "    loss = closure()\n",
        "    epoch_loss += loss.item()\n",
        "\n",
        "  print(\"Epoch: {} Loss: {}\".format(epoch,epoch_loss/len(trainloader)))\n",
        "\n"
      ],
      "metadata": {
        "id": "45lkIbuzIVNE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93a4e498-7fbc-4fd1-cab6-4a031864d65c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 Loss: 0.05615426052015457\n",
            "Epoch: 1 Loss: 0.0\n",
            "Epoch: 2 Loss: 0.0\n",
            "Epoch: 3 Loss: 0.0\n",
            "Epoch: 4 Loss: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model(train_data[2][0].unsqueeze(0).to(device))\n",
        "print(f\" Model predicted label : {np.argmax(y_pred.detach().cpu().numpy())}\")\n",
        "print(f\" Actual label : {train_data[2][1]}\")"
      ],
      "metadata": {
        "id": "Bz8h1KFp0dWn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ff38856-de97-465b-bdfe-9938a2ae1e79"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Model predicted label : 1\n",
            " Actual label : 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bLgdXworw2F4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}