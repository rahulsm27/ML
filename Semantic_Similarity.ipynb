{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMjkOuFdpu3W1DWMxTaBdHC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahulsm27/ML/blob/main/Semantic_Similarity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. DOC2VEC"
      ],
      "metadata": {
        "id": "vKl7AlwR8KBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYTApj9X7nDo",
        "outputId": "ebcbece3-8e56-464f-f7f0-780a281bfec2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document 2: Similarity Score: 0.9834631085395813\n",
            "Document Text: The baby learned to walk in the 5th month itself\n",
            "\n",
            "Document 0: Similarity Score: 0.9460567831993103\n",
            "Document Text: The movie is awesome. It was a good thriller\n",
            "\n",
            "Document 1: Similarity Score: 0.8827896118164062\n",
            "Document Text: We are learning NLP throughg GeeksforGeeks\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Sample data\n",
        "data = [\"The movie is awesome. It was a good thriller\",\n",
        "        \"We are learning NLP throughg GeeksforGeeks\",\n",
        "        \"The baby learned to walk in the 5th month itself\"]\n",
        "\n",
        "# Tokenizing the data\n",
        "tokenized_data = [word_tokenize(document.lower()) for document in data]\n",
        "\n",
        "# Creating TaggedDocument objects\n",
        "tagged_data = [TaggedDocument(words=words, tags=[str(idx)]) for idx, words in enumerate(tokenized_data)]\n",
        "\n",
        "\n",
        "# Training the Doc2Vec model\n",
        "model = Doc2Vec(vector_size=100, window=2, min_count=1, workers=4, epochs=1000)\n",
        "model.build_vocab(tagged_data)\n",
        "model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
        "\n",
        "# Infer vector for a new document\n",
        "new_document = \"The baby was laughing and palying\"\n",
        "inferred_vector = model.infer_vector(word_tokenize(new_document.lower()))\n",
        "\n",
        "# Find most similar documents\n",
        "similar_documents = model.dv.most_similar([inferred_vector], topn=len(model.dv))\n",
        "\n",
        "# Print the most similar documents\n",
        "for index, score in similar_documents:\n",
        "    print(f\"Document {index}: Similarity Score: {score}\")\n",
        "    print(f\"Document Text: {data[int(index)]}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2 SBERT"
      ],
      "metadata": {
        "id": "ItvLvB3Arn5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U sentence-transformers"
      ],
      "metadata": {
        "id": "R58ssVFyXnX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install -U sentence-transformers\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Sample sentence\n",
        "sentences = [\"The movie is awesome. It was a good thriller\",\n",
        "        \"We are learning NLP throughg GeeksforGeeks\",\n",
        "        \"The baby learned to walk in the 5th month itself\"]\n",
        "\n"
      ],
      "metadata": {
        "id": "wFiZooF-9X1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.spatial import distance\n",
        "\n",
        "\n",
        "test = \"I liked the movie\"\n",
        "test_vec = model.encode([test])[0]\n",
        "\n",
        "\n",
        "for sent in sentences:\n",
        "  similarity_score = 1-distance.cosine(test_vec,model.encode([sent])[0])\n",
        "  print(f'similarity = {similarity_score} for {sent}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83Q2NSjjXlvS",
        "outputId": "5672ee7c-a6f9-4fd6-bb97-1a38af8ee045"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "similarity = 0.6631895899772644 for The movie is awesome. It was a good thriller\n",
            "similarity = 0.10694186389446259 for We are learning NLP throughg GeeksforGeeks\n",
            "similarity = 0.04317505657672882 for The baby learned to walk in the 5th month itself\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "a = np.array([1.0,2.0,3.0,4.0])\n",
        "b= np.array([3.0,4.0,5.0,6.0])\n",
        "# cos_sim = dot(a, b)/(norm(a)*norm(b))\n",
        "# cos_sim"
      ],
      "metadata": {
        "id": "rChEmLSdYjVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. INFERSENT"
      ],
      "metadata": {
        "id": "Wy5Yjo00rt5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline\n",
        "from random import randint\n",
        "import numpy as np\n",
        "import torch\n",
        "import time\n",
        "import torch.nn as nn\n",
        "class InferSent(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super(InferSent, self).__init__()\n",
        "        self.bsize = config['bsize']\n",
        "        self.word_emb_dim = config['word_emb_dim']\n",
        "        self.enc_lstm_dim = config['enc_lstm_dim']\n",
        "        self.pool_type = config['pool_type']\n",
        "        self.dpout_model = config['dpout_model']\n",
        "        self.version = 1 if 'version' not in config else config['version']\n",
        "\n",
        "        self.enc_lstm = nn.LSTM(self.word_emb_dim, self.enc_lstm_dim, 1,\n",
        "                                bidirectional=True, dropout=self.dpout_model)\n",
        "\n",
        "        assert self.version in [1, 2]\n",
        "        if self.version == 1:\n",
        "            self.bos = '<s>'\n",
        "            self.eos = '</s>'\n",
        "            self.max_pad = True\n",
        "            self.moses_tok = False\n",
        "        elif self.version == 2:\n",
        "            self.bos = '<p>'\n",
        "            self.eos = '</p>'\n",
        "            self.max_pad = False\n",
        "            self.moses_tok = True\n",
        "\n",
        "    def is_cuda(self):\n",
        "        # either all weights are on cpu or they are on gpu\n",
        "        return self.enc_lstm.bias_hh_l0.data.is_cuda\n",
        "\n",
        "    def forward(self, sent_tuple):\n",
        "        # sent_len: [max_len, ..., min_len] (bsize)\n",
        "        # sent: (seqlen x bsize x worddim)\n",
        "        sent, sent_len = sent_tuple\n",
        "\n",
        "        # Sort by length (keep idx)\n",
        "        sent_len_sorted, idx_sort = np.sort(sent_len)[::-1], np.argsort(-sent_len)\n",
        "        sent_len_sorted = sent_len_sorted.copy()\n",
        "        idx_unsort = np.argsort(idx_sort)\n",
        "\n",
        "        idx_sort = torch.from_numpy(idx_sort).cuda() if self.is_cuda() \\\n",
        "            else torch.from_numpy(idx_sort)\n",
        "        sent = sent.index_select(1, idx_sort)\n",
        "\n",
        "        # Handling padding in Recurrent Networks\n",
        "        sent_packed = nn.utils.rnn.pack_padded_sequence(sent, sent_len_sorted)\n",
        "        sent_output = self.enc_lstm(sent_packed)[0]  # seqlen x batch x 2*nhid\n",
        "        sent_output = nn.utils.rnn.pad_packed_sequence(sent_output)[0]\n",
        "\n",
        "        # Un-sort by length\n",
        "        idx_unsort = torch.from_numpy(idx_unsort).cuda() if self.is_cuda() \\\n",
        "            else torch.from_numpy(idx_unsort)\n",
        "        sent_output = sent_output.index_select(1, idx_unsort)\n",
        "\n",
        "        # Pooling\n",
        "        if self.pool_type == \"mean\":\n",
        "            sent_len = torch.FloatTensor(sent_len.copy()).unsqueeze(1).cuda()\n",
        "            emb = torch.sum(sent_output, 0).squeeze(0)\n",
        "            emb = emb / sent_len.expand_as(emb)\n",
        "        elif self.pool_type == \"max\":\n",
        "            if not self.max_pad:\n",
        "                sent_output[sent_output == 0] = -1e9\n",
        "            emb = torch.max(sent_output, 0)[0]\n",
        "            if emb.ndimension() == 3:\n",
        "                emb = emb.squeeze(0)\n",
        "                assert emb.ndimension() == 2\n",
        "\n",
        "        return emb\n",
        "\n",
        "    def set_w2v_path(self, w2v_path):\n",
        "        self.w2v_path = w2v_path\n",
        "\n",
        "    def get_word_dict(self, sentences, tokenize=True):\n",
        "        # create vocab of words\n",
        "        word_dict = {}\n",
        "        sentences = [s.split() if not tokenize else self.tokenize(s) for s in sentences]\n",
        "        for sent in sentences:\n",
        "            for word in sent:\n",
        "                if word not in word_dict:\n",
        "                    word_dict[word] = ''\n",
        "        word_dict[self.bos] = ''\n",
        "        word_dict[self.eos] = ''\n",
        "        return word_dict\n",
        "\n",
        "    def get_w2v(self, word_dict):\n",
        "        assert hasattr(self, 'w2v_path'), 'w2v path not set'\n",
        "        # create word_vec with w2v vectors\n",
        "        word_vec = {}\n",
        "        with open(self.w2v_path) as f:\n",
        "            for line in f:\n",
        "                word, vec = line.split(' ', 1)\n",
        "                if word in word_dict:\n",
        "                    word_vec[word] = np.fromstring(vec, sep=' ')\n",
        "        print('Found %s(/%s) words with w2v vectors' % (len(word_vec), len(word_dict)))\n",
        "        return word_vec\n",
        "\n",
        "    def get_w2v_k(self, K):\n",
        "        assert hasattr(self, 'w2v_path'), 'w2v path not set'\n",
        "        # create word_vec with k first w2v vectors\n",
        "        k = 0\n",
        "        word_vec = {}\n",
        "        with open(self.w2v_path) as f:\n",
        "            for line in f:\n",
        "                word, vec = line.split(' ', 1)\n",
        "                if k <= K:\n",
        "                    word_vec[word] = np.fromstring(vec, sep=' ')\n",
        "                    k += 1\n",
        "                if k > K:\n",
        "                    if word in [self.bos, self.eos]:\n",
        "                        word_vec[word] = np.fromstring(vec, sep=' ')\n",
        "\n",
        "                if k > K and all([w in word_vec for w in [self.bos, self.eos]]):\n",
        "                    break\n",
        "        return word_vec\n",
        "\n",
        "    def build_vocab(self, sentences, tokenize=True):\n",
        "        assert hasattr(self, 'w2v_path'), 'w2v path not set'\n",
        "        word_dict = self.get_word_dict(sentences, tokenize)\n",
        "        self.word_vec = self.get_w2v(word_dict)\n",
        "        print('Vocab size : %s' % (len(self.word_vec)))\n",
        "\n",
        "    # build w2v vocab with k most frequent words\n",
        "    def build_vocab_k_words(self, K):\n",
        "        assert hasattr(self, 'w2v_path'), 'w2v path not set'\n",
        "        self.word_vec = self.get_w2v_k(K)\n",
        "        print('Vocab size : %s' % (K))\n",
        "\n",
        "    def update_vocab(self, sentences, tokenize=True):\n",
        "        assert hasattr(self, 'w2v_path'), 'warning : w2v path not set'\n",
        "        assert hasattr(self, 'word_vec'), 'build_vocab before updating it'\n",
        "        word_dict = self.get_word_dict(sentences, tokenize)\n",
        "\n",
        "        # keep only new words\n",
        "        for word in self.word_vec:\n",
        "            if word in word_dict:\n",
        "                del word_dict[word]\n",
        "\n",
        "        # udpate vocabulary\n",
        "        if word_dict:\n",
        "            new_word_vec = self.get_w2v(word_dict)\n",
        "            self.word_vec.update(new_word_vec)\n",
        "        else:\n",
        "            new_word_vec = []\n",
        "        print('New vocab size : %s (added %s words)'% (len(self.word_vec), len(new_word_vec)))\n",
        "\n",
        "    def get_batch(self, batch):\n",
        "        # sent in batch in decreasing order of lengths\n",
        "        # batch: (bsize, max_len, word_dim)\n",
        "        embed = np.zeros((len(batch[0]), len(batch), self.word_emb_dim))\n",
        "\n",
        "        for i in range(len(batch)):\n",
        "            for j in range(len(batch[i])):\n",
        "                embed[j, i, :] = self.word_vec[batch[i][j]]\n",
        "\n",
        "        return torch.FloatTensor(embed)\n",
        "\n",
        "    def tokenize(self, s):\n",
        "        from nltk.tokenize import word_tokenize\n",
        "        if self.moses_tok:\n",
        "            s = ' '.join(word_tokenize(s))\n",
        "            s = s.replace(\" n't \", \"n 't \")  # HACK to get ~MOSES tokenization\n",
        "            return s.split()\n",
        "        else:\n",
        "            return word_tokenize(s)\n",
        "\n",
        "    def prepare_samples(self, sentences, bsize, tokenize, verbose):\n",
        "        sentences = [[self.bos] + s.split() + [self.eos] if not tokenize else\n",
        "                     [self.bos] + self.tokenize(s) + [self.eos] for s in sentences]\n",
        "        n_w = np.sum([len(x) for x in sentences])\n",
        "\n",
        "        # filters words without w2v vectors\n",
        "        for i in range(len(sentences)):\n",
        "            s_f = [word for word in sentences[i] if word in self.word_vec]\n",
        "            if not s_f:\n",
        "                import warnings\n",
        "                warnings.warn('No words in \"%s\" (idx=%s) have w2v vectors. \\\n",
        "                               Replacing by \"</s>\"..' % (sentences[i], i))\n",
        "                s_f = [self.eos]\n",
        "            sentences[i] = s_f\n",
        "\n",
        "        lengths = np.array([len(s) for s in sentences])\n",
        "        n_wk = np.sum(lengths)\n",
        "        if verbose:\n",
        "            print('Nb words kept : %s/%s (%.1f%s)' % (\n",
        "                        n_wk, n_w, 100.0 * n_wk / n_w, '%'))\n",
        "\n",
        "        # sort by decreasing length\n",
        "        lengths, idx_sort = np.sort(lengths)[::-1], np.argsort(-lengths)\n",
        "        sentences = np.array(sentences)[idx_sort]\n",
        "\n",
        "        return sentences, lengths, idx_sort\n",
        "\n",
        "    def encode(self, sentences, bsize=64, tokenize=True, verbose=False):\n",
        "        tic = time.time()\n",
        "        sentences, lengths, idx_sort = self.prepare_samples(\n",
        "                        sentences, bsize, tokenize, verbose)\n",
        "\n",
        "        embeddings = []\n",
        "        for stidx in range(0, len(sentences), bsize):\n",
        "            batch = self.get_batch(sentences[stidx:stidx + bsize])\n",
        "            if self.is_cuda():\n",
        "                batch = batch.cuda()\n",
        "            with torch.no_grad():\n",
        "                batch = self.forward((batch, lengths[stidx:stidx + bsize])).data.cpu().numpy()\n",
        "            embeddings.append(batch)\n",
        "        embeddings = np.vstack(embeddings)\n",
        "\n",
        "        # unsort\n",
        "        idx_unsort = np.argsort(idx_sort)\n",
        "        embeddings = embeddings[idx_unsort]\n",
        "\n",
        "        if verbose:\n",
        "            print('Speed : %.1f sentences/s (%s mode, bsize=%s)' % (\n",
        "                    len(embeddings)/(time.time()-tic),\n",
        "                    'gpu' if self.is_cuda() else 'cpu', bsize))\n",
        "        return embeddings\n",
        "\n",
        "    def visualize(self, sent, tokenize=True):\n",
        "\n",
        "        sent = sent.split() if not tokenize else self.tokenize(sent)\n",
        "        sent = [[self.bos] + [word for word in sent if word in self.word_vec] + [self.eos]]\n",
        "\n",
        "        if ' '.join(sent[0]) == '%s %s' % (self.bos, self.eos):\n",
        "            import warnings\n",
        "            warnings.warn('No words in \"%s\" have w2v vectors. Replacing \\\n",
        "                           by \"%s %s\"..' % (sent, self.bos, self.eos))\n",
        "        batch = self.get_batch(sent)\n",
        "\n",
        "        if self.is_cuda():\n",
        "            batch = batch.cuda()\n",
        "        output = self.enc_lstm(batch)[0]\n",
        "        output, idxs = torch.max(output, 0)\n",
        "        # output, idxs = output.squeeze(), idxs.squeeze()\n",
        "        idxs = idxs.data.cpu().numpy()\n",
        "        argmaxs = [np.sum((idxs == k)) for k in range(len(sent[0]))]\n",
        "\n",
        "        # visualize model\n",
        "        import matplotlib.pyplot as plt\n",
        "        plt.figure(figsize=(12,12))\n",
        "        x = range(len(sent[0]))\n",
        "        y = [100.0 * n / np.sum(argmaxs) for n in argmaxs]\n",
        "        plt.xticks(x, sent[0], rotation=45)\n",
        "        plt.bar(x, y)\n",
        "        plt.ylabel('%')\n",
        "        plt.title('Visualisation of words importance')\n",
        "        plt.show()\n",
        "\n",
        "        return output, idxs, argmaxs"
      ],
      "metadata": {
        "id": "GhJeFELghqxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir fastText\n",
        "!curl -Lo fastText/crawl-300d-2M.vec.zip https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip\n",
        "!unzip fastText/crawl-300d-2M.vec.zip -d fastText/\n",
        "\n",
        "!mkdir encoder\n",
        "!curl -Lo encoder/infersent2.pkl https://dl.fbaipublicfiles.com/infersent/infersent2.pkl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSOLvO4LoUCd",
        "outputId": "3b6a8bdc-3219-4429-d7e6-916dda79d018"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 1453M  100 1453M    0     0   122M      0  0:00:11  0:00:11 --:--:-- 98.3M\n",
            "Archive:  fastText/crawl-300d-2M.vec.zip\n",
            "  inflating: fastText/crawl-300d-2M.vec  \n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  146M  100  146M    0     0  95.1M      0  0:00:01  0:00:01 --:--:-- 95.1M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BDvatYAoT-r",
        "outputId": "30c097b4-521e-467a-aa80-7711c577ab10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "MODEL_PATH = 'encoder/infersent2.pkl'\n",
        "params_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,\n",
        "                'pool_type': 'max', 'dpout_model': 0.0, 'version': V}\n",
        "model = InferSent(params_model)\n",
        "model.load_state_dict(torch.load(MODEL_PATH))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBcEpfLho9qt",
        "outputId": "3984b10d-5db4-49c1-db40-492ee664a04c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "W2V_PATH = 'fastText/crawl-300d-2M.vec'\n",
        "model.set_w2v_path(W2V_PATH)"
      ],
      "metadata": {
        "id": "3Ea502GBqMLr",
        "outputId": "0545ca65-a6d9-4165-e961-cbf4902a06b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'_UserObject' object has no attribute 'set_w2v_path'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-06ddc1c946bd>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mW2V_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'fastText/crawl-300d-2M.vec'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_w2v_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW2V_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: '_UserObject' object has no attribute 'set_w2v_path'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load embeddings of K most frequent words\n",
        "model.build_vocab_k_words(K=100000)"
      ],
      "metadata": {
        "id": "ke3M5mWkvoD7",
        "outputId": "418993be-333d-4655-9721-99070f34bff1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size : 100000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VYDCsJ23voCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample sentence\n",
        "sentences = [\"The movie is awesome. It was a good thriller\",\n",
        "        \"We are learning NLP throughg GeeksforGeeks\",\n",
        "        \"The baby learned to walk in the 5th month itself\"]"
      ],
      "metadata": {
        "id": "7X8Zdm_Es_cL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.spatial import distance\n",
        "\n",
        "\n",
        "test = \"I liked the movie\"\n",
        "test_vec = model.encode([test])[0]\n",
        "\n",
        "\n",
        "for sent in sentences:\n",
        "#  similarity_score= dot(test_vec, model.encode([sent])[0])/(norm(test_vec)*model.encode([sent])[0])\n",
        "  similarity_score = 1-distance.cosine(test_vec,model.encode([sent])[0])\n",
        "  print(f'similarity = {similarity_score} for {sent}')"
      ],
      "metadata": {
        "id": "q9P9owiutDRp",
        "outputId": "1b37abe9-2fc5-46b4-db07-3f331f40d206",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "similarity = 0.5316413044929504 for The movie is awesome. It was a good thriller\n",
            "similarity = 0.33099278807640076 for We are learning NLP throughg GeeksforGeeks\n",
            "similarity = 0.1230548769235611 for The baby learned to walk in the 5th month itself\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# USE"
      ],
      "metadata": {
        "id": "2hcVyU0g6JJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import seaborn as sns\n",
        "\n",
        "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
        "model = hub.load(module_url)\n",
        "print (\"module %s loaded\" % module_url)\n",
        "def embed(input):\n",
        "  return model(input)"
      ],
      "metadata": {
        "id": "xK59EslSu1_8",
        "outputId": "cff3f71d-9c9c-4e65-ba38-c8b707bcf03d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "module https://tfhub.dev/google/universal-sentence-encoder/4 loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.spatial import distance\n",
        "\n",
        "\n",
        "test = [\"I liked the movie very much\"]\n",
        "test_vec = embed(test)\n",
        "# Sample sentence\n",
        "sentences = [[\"The movie is awesome and It was a good thriller\"],\n",
        "        [\"We are learning NLP throughg GeeksforGeeks\"],\n",
        "        [\"The baby learned to walk in the 5th month itself\"]]\n",
        "\n",
        "for sent in sentences:\n",
        "  similarity_score = 1-distance.cosine(test_vec[0,:],embed(sent)[0,:])\n",
        "  print(f'similarity = {similarity_score} for {sent}')"
      ],
      "metadata": {
        "id": "ZpZnguBs2E_9",
        "outputId": "20ede15c-ccc4-4502-f3d7-4f67675ecbe2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "similarity = 0.6519516706466675 for ['The movie is awesome and It was a good thriller']\n",
            "similarity = 0.06988029181957245 for ['We are learning NLP throughg GeeksforGeeks']\n",
            "similarity = -0.011212967336177826 for ['The baby learned to walk in the 5th month itself']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test = [\"I liked the movie\"]\n",
        "test_vec = embed(test)[0,:]"
      ],
      "metadata": {
        "id": "7zHgfS212as4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bbrixNO643aQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}